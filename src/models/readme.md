# Extracting Data
This repo is for recording the progress for creating a dataset for a set of latitudes and longitudes as described [here](https://drive.google.com/file/d/1QPhcLANBwB0gjdIHsv5ohU-mcAwClh9j/view?usp=sharing)

## Viz1.py
In this code, we are just visualising the Brazilian Amazon. We do this using a file called Limites_Amazonia_Legal_2022.shp, which I downloaded from [IBGE](https://www.ibge.gov.br/geociencias/organizacao-do-territorio/estrutura-territorial/15819-amazonia-legal.html), which is the primary provider for statistical and geographical information for Brazil. So in this code, we just print the head of the .shp file, the CRS(Coordinate Reference System) and we plot the points.

### Results:
```
NOME     AREA_KM2                                           geometry
0  Amazônia Legal  5015145.999  MULTIPOLYGON (((-45.61083 -1.27461, -45.60796 ...
````
As you can see. this file only consists of one value with the entire geometry of Brazilian Amazon

EPSG:4674  --->  This is the coordinaate reference system of the downloaded file, it basically tells geopandas how to associate the points in this file to actual places on earth.

![image](https://github.com/user-attachments/assets/0d89eca2-d79e-41e5-b603-495191f3c6de)


## Griddy.py
This file takes the entire map from the Limites_Amazonia_Legal_2022.shp file, and divides it into smaller box grids, each of area 36 sq.km (6000 m X 6000 m squares). It finally writes the new boxes to a file called amazon_grid.shp, which will be visualised in viz2.py. A total of 142401 boxes were generated.
To divide the file into grids of equal size (area = 36sq. km), we change the CRS from EPSG:4674 to EPSG:5880
Why are we reprojecting to EPSG:5880?
The original file was in A geographic CRS (EPSG:4674) where the coordinates are in decimal degrees. This makes the distance and area calculations non uniform and distorted. EPSG:5880 is a Brazilian equal area projection CRS (unit in metres), so if we say we want to travel 500 metres, it actually helps us travel 500 metres. The calculations become more accurate. It helps us build a perfectly regular grid in metric units. 


## viz2.py

same thing as viz1.py, prints the head and plots the points. The CRS is different here (EPSG:5880), as it was changed by the griddy.py file. 

### Results:
             NOME     AREA_KM2                                           geometry
0  Amazônia Legal  5015145.999  POLYGON ((2800536 9114218, 2800536 9108883.768...
1  Amazônia Legal  5015145.999  POLYGON ((2800536 9120218, 2800536 9114218, 27...
2  Amazônia Legal  5015145.999  POLYGON ((2800536 9120218, 2800238.293 9120218...
3  Amazônia Legal  5015145.999  POLYGON ((2800536 9132218, 2800536 9129555.408...
4  Amazônia Legal  5015145.999  POLYGON ((2800536 9132218, 2798766.37 9132218,...


![image](https://github.com/user-attachments/assets/6f881e5f-33e3-4908-94d9-9aabdcab4b95)


## finding_centroids.py

As the name describes, this code is for finding the centroids of each of the 142401 grids generated before. We will be extracting the data for these points. These points are stored into a new file called amazon_centroids.shp. Here, the coordinates are in EPSG:5880



## finding_more_points.py

Later on, we realised that centroids of 36 sq.km cannot help in accurately judging weather a particular location could be a potential site with a geoglyph. So we went into each grid, and found coordinates of points that are 500 metres away from each other. This resulted in 17 million coordinates across Brazilian Amazon.
So, we take the amazon_centroids.shp file generated by the previous code and generate a new file called amazon_500m_samples.shp. Even here, all the coordinates are in EPSG:5880

I checked the CRS (Coordinate Reference System) of the tif files that I obtained from WorldClim (the ones I showed yall on meet - the world maps), it was EPSG:4326, which is actual latitudes and longitude format that we are aware of. So, I will be converting this new file into another one with coordinates in latitude and longitude form

## converting_to_4326.py
Helps in converting the crs of the longitudes and latitudes to EPSG:4326 and save them to a file called amazon_500m_latlon.shp

## saving_latlon_to_csv.py
Converts the amazon_500m_latlon.shp file to amazon_500m_features.csv

## extracting_data.py
This code deals with extracting the bioclimatic variables from the [worldclim dataset](https://worldclim.org/data/worldclim21.html). It also extracts elevation data from a dataset I found on [kaggle](https://www.kaggle.com/datasets/minervasdatalab/amazon-basin-dem). So we are taking 7 tif files from the worldclim dataset and 1 tif file from the dataset on kaggle. Each tif file represents a variable, like elevation, temperature seasonality, etc. We extract data for each of these variables by feeding the coordinates that we found earlier. The reason why we converted all our coordinates above to EPSG:4326 is because these tif files are in that CRS. It writes the new CSV file with the latitude, longitude and the new columns into a file called amazon_500m_with_worldclim_vars.csv

## Soil data
To get the remaining two variables, sand fraction and gravel content, we used the [hwsd dataset](https://www.fao.org/soils-portal/data-hub/soil-maps-and-databases/harmonized-world-soil-database-v20/en/). This was a bit tricky. Firstly, the files downloaded from here is a bil file, not a tif file and that too, in a different crs than EPSG:4326. So first, we had to write a script to convert the bil file's crs and store it in a tif file. Another tricky part is, when we enter a pair of coordinates, we dont get data for a particular variable, we only get a mu_key(map unit key), which is a primary column in an associated database called hwsd.csv. This csv file contains several soil data variables and the value for each mu_key, which is the primary column. So the following two scripts perform the tasks mentioned here

## bil_data_to_4326.py
converts hwsd2.bil's crs to 4326 aqnd reporjects the data to a tif file called HWSD2_Map_Unit_4326.tif

## extract_soildata.py
extracts soil data for each coordinate by extracting the corresponding mu_key from the tif file and then using this mu_key in the hwsd_data.csv to get the required variables - soil fraction and gravel content. Another tricky part here is that in the hwsd_data.csv, there are multiple rows with the same mu_key, so we take the average of values for a particular mu_key.
This code edits the amazon_500m_worldclim_vars.csv file generated by the extracting_data.py file earlier. After adding the required columns, it writes everything TO amazon_unknown_geoglyph_points.csv


## Known Points
Till now, we have been extracting data for all the coordinates across Brazilian Amazon. Now, we also have to collect the same data for the points that have been discovered as geoglyphs earlier. We have the coordinates of such points stored in amazon_geoglyph_coords.csv. This file was obtained from <Tanay, link the source here>. Now we run the same scripts we ran earlier to extract the data for these coordinates. That is done in the upcoming files - extracting_data_for_known.py and extracting_soildata_for_known.py

## extracting_data_for_known.py
same code as extract_data.py, but here, it reads from amazon_geoglyph_coords.csv and writes to amazon_known_geoglyph_data.csv

## extracting_soildata_for_known.py
same code as extract_soildata.py, except here, it edits the amazon_known_geoglyph_data.py


